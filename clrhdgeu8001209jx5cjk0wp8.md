---
title: "Le Surapprentissage ou Surajustement (overfitting) en MachineÂ Learning"
datePublished: Wed Jan 17 2024 05:58:14 GMT+0000 (Coordinated Universal Time)
cuid: clrhdgeu8001209jx5cjk0wp8
slug: le-surapprentissage-ou-surajustement-overfitting-en-machine-learning
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1705471135875/4dc9f028-82a7-446f-ba46-c10202e9d951.png
ogImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1705471055241/70aa508a-f8be-4d8a-bc0b-06791943c79f.png
tags: optimization, data-science, machine-learning, deep-learning, overfitting

---

![Source: Wikipediaâ€Šâ€”â€ŠOverfitting](https://cdn-images-1.medium.com/max/800/1*3wSW_r5fSSTzxCLurfk5Zg.png align="center")

Nous, les humains, avons trÃ¨s souvent tendance Ã  surgÃ©nÃ©raliser les faits dont nous sommes tÃ©moin. Par exemple, lorsque nous arrivons tout nouvellement en terre Ã©trangÃ¨re et quâ€™un taximan ou un boutiquier nous escroque, nous serions, Ã  priori, tentÃ©s dans cette situation dâ€™affirmer que tous les taximen ou les boutiquiers de ce pays sont des â€œvoleursâ€. Cette tendance Ã  faire des inductions hatives, câ€™est-Ã -dire Ã  dÃ©duire une rÃ¨gle gÃ©nÃ©rale Ã  partir dâ€™une simple observation conduit trÃ¨s souvent Ã  des conclusions incorrectes.

Eh bien, figurez vous que tout comme nous, câ€™est malheureusement un piÃ¨ge dans lequel tombent Ã©galement les algorithmes de Machine Learning quand nous ne sommes pas vigilants lors de leur entrainement. ***Câ€™est cette situation de surgÃ©nÃ©ralisation quâ€™on appelle Overfitting (ou surapprentissage ou encore surajustement en franÃ§ais).***

Dans cet article, nous allons aborder en dÃ©tails, les points suivantsÂ :

1. DÃ©finition de lâ€™Overfitting
    
2. Les causes de lâ€™Overfitting
    
3. Un cas pratique dâ€™Overfitting
    
4. Une explication plus courante de lâ€™Overfitting
    
5. Comment Ã©viter lâ€™Overfitting?
    
6. Le principe de simplicitÃ© ou Razoir dâ€™Occam
    

### 1\. DÃ©finition de lâ€™Overfitting

> *On parle dâ€™Overfitting quand un algorithme de Machine Learning performe trÃ¨s trÃ¨s bien sur les donnÃ©es dâ€™apprentissage quâ€™on lui fournit (training set) mais obtient relativement de mauvaises performances lorquâ€™il est testÃ© sur de nouveaux exemples (test set) quâ€™il nâ€™a jamais vus.*

En effet, lorsquâ€™un modÃ¨le est trop complexe et sâ€™ajuste trop Ã©troitement aux donnÃ©es dâ€™entraÃ®nement, capturant mÃªme le bruit et les erreurs dans ces donnÃ©es, cela peut conduire Ã  une ***mauvaise gÃ©nÃ©ralisation du modÃ¨le sur de nouvelles donnÃ©es, car il est trop spÃ©cialisÃ© pour les donnÃ©es dâ€™entraÃ®nement spÃ©cifiques.***

En francais facile, il y a un risque de tirer des conclusions trop hÃ¢tives Ã  partir dâ€™un ensemble limitÃ© dâ€™informations, ce qui peut conduire Ã  des erreurs lorsque ces conclusions sont appliquÃ©es Ã  des situations plus gÃ©nÃ©rales ou Ã  de nouvelles donnÃ©es.

![Sourceâ€Šâ€”â€Šhttps://www.freecodecamp.org/](https://cdn-images-1.medium.com/max/800/0*guBo5AYxPB4PI9o3.jpg align="left")

Nous verrons plutard une autre explication plus courante mais pour le moment la question que lâ€™on doit se poser câ€™est ***quâ€™est-ce qui est Ã  lâ€™origine de lâ€™overfitting?***

### 2\. Les causes de lâ€™Overfitting (non exhaustif)

GÃ©nÃ©ralement lâ€™overfitting est liÃ© soit au modÃ¨le ou soit aux donnÃ©es dâ€™entraÃ®nement:

* **Un modÃ¨le un peu trop complexe** avec un grand nombre de paramÃ¨tres ou une capacitÃ© dâ€™ajustement Ã©levÃ©e, peut Ãªtre trop flexible et ajuster les donnÃ©es dâ€™entraÃ®nement de maniÃ¨re trop prÃ©cise.
    
* **Un manque (peu) de donnÃ©es dâ€™entraÃ®nement** peut empÃªcher le modÃ¨le de capturer les variations rÃ©elles du dataset et donc il va se concentrer sur des dÃ©tails spÃ©cifiques(souvent inutiles) qui ne se gÃ©nÃ©ralisent pas bien Ã  de nouvelles donnÃ©es.
    
* **Une mauvaise qualitÃ© des donnÃ©es (prÃ©sence de caractÃ©ristiques redondantes ou inutiles)** peut contribuer Ã  lâ€™overfitting car certaines informations peuvent Ãªtre non pertinentes et ne gÃ©nÃ©ralisent pas vraiment Ã  de nouvelles donnÃ©es.
    

Câ€™est le cas par exemple lorquâ€™on a des valeurs abÃ©rrantes (outliers or noisy data points) dans le dataset.

![Source: TowardsAI](https://cdn-images-1.medium.com/max/800/1*AfKsExgtYR7yfPjZvciKXQ.png align="left")

Bien. Maintenant que nous avons une idÃ©e plus ou moins claire de ce que câ€™est que lâ€™overfitting et de ses causes, voyons concrÃ¨tement par un exemple comment il se manifeste.

### 3\. Un cas pratique dâ€™Overfitting

Pour cet exemple, nous allons effectuer une classification binaire avec un ensemble de donnÃ©es sur la *prÃ©vision du taux dâ€™attrition(dÃ©sabonnement) de la clientÃ¨le des banques (Bank Churn Dataset)* que vous pouvez tÃ©lÃ©charcher sur Kaggle ou directement Ã  partir du lien que je mettrai en fin dâ€™article.

CommenÃ§ons par importer les librairies de base dont nous aurons besoin. Nous importerons le reste au fur et Ã  mesure que lâ€™on avance.

```python
# importer les librairies necessaires
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

# ajuster la taille du texte sur les graphiques
matplotlib.rc('xtick', labelsize=15)
matplotlib.rc('ytick', labelsize=15)

# pour afficher les graphiques dans le notebook lui meme
%matplotlib inline
```

Puis on importe le fichier csv Ã  lâ€™aide de Pandas et on affiche les dix (10) premiÃ¨res lignes pour avoir une idÃ©e globale du dataset. Notez que jâ€™utilise un notebook Kaggle pour cet exemple.

```python
# importer le fichier csv
df = pd.read_csv("/kaggle/input/playground-series-s4e1/train.csv")

# afficher les 10 premieres lignes du dataset
df.head(10)
```

![](https://cdn-images-1.medium.com/max/800/1*XsJrEBEWtwRB3rZFDEKZ0w.jpeg align="left")

***Petit apercu du Datasetâ€Šâ€”â€ŠBank ChurnÂ Dataset***

Nous avons en face un problÃ¨me de classification binaire ou la variable cible (target) est la colonne â€˜Exitedâ€™. Comme mÃ©trique dâ€™Ã©valuation, nous allons utiliser lâ€™[*area under the ROC curve*](https://fr.wikipedia.org/wiki/Courbe_ROC) ou score AUCÂ .Â   
Dans un but de simplicitÃ©, nous nâ€™allons utiliser que quelques variables de ce dataset et on ne fera pas dâ€™analyse exploratoire de donnÃ©es ou de feature engineering. Libre Ã  vous de jetter un coup dâ€™oeil rapide au dataset pour vous imprÃ©gner de la distribution des variables.

Par contre une Ã©tape essentielle avant dâ€™atteindre notre but est de tranformer les variables catÃ©gorielles quâ€™on veut utiliser (notamment les variables â€˜Geographyâ€™ et â€˜Genderâ€™) en variables numÃ©riques:

```python
# transformation des variables catÃ©gorielles en variables numÃ©riques
mapping_geography = {"France": 0, "Spain": 1, "Germany": 2}
mapping_gender    = {"Male": 0, "Female": 1}

# mapping
df.loc[:, "Geography"] = df.Geography.map(mapping_geography)
df.loc[:, "Gender"]    = df.Gender.map(mapping_gender)
```

Une fois les variables catÃ©gorielles mappÃ©es correctement(je ne sais pas si le verbe â€˜mapperâ€™ existe en francais:) ) et transformÃ©es, nous allons diviser le dataset en **train set**(donnÃ©es dâ€™entrainement) et en **test set**(donnÃ©es dâ€™Ã©valuation). Retenez bien ces deux noms (train set et test set) car ils seront beaucoup utilisÃ©s par la suite.

```python
from sklearn.model_selection import train_test_split

# nous allons utiliser ces 10 colonnes
features = ["CreditScore", "Age", "Tenure", "Balance", "NumOfProducts",
 "HasCrCard", "IsActiveMember", "EstimatedSalary", "Geography", "Gender"]

X = df[features]    # features
y = df.Exited       # variable cible

# Division des donnÃ©es en train et test set
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2024)
```

Enfin nous pouvons entrainer un premier modÃ¨le de Machine Learning sur ce dataset. Pour simplifier, choisissons un algorithme simple et classique: un arbre de decision (decision tree) avec une profondeur maximale de 3 et les autres paramÃ¨tres laissÃ©s Ã  leurs valeurs par dÃ©faut:

```python
from sklearn.tree import DecisionTreeClassifier

# initialiser l'arbre de decision avec le parametre max_depth a 3
clf = DecisionTreeClassifier(max_depth=3)

# entrainer le modele sur les colonnes choisies plus haut
clf.fit(X_train, y_train)
```

```python
from sklearn.metrics import roc_auc_score # mÃ©trique AUC

# faire des predictions sur le training set et sur le test set
train_predictions = clf.predict(X_train)
test_predictions  = clf.predict(X_test)

# caluculer le score des predictions
train_auc = roc_auc_score(y_train, train_predictions)
test_auc  = roc_auc_score(y_test,  test_predictions)

print(f"Le score AUC sur le train set    ----> {train_auc}")
print(f"Le score AUC sur le test set     ----> {test_auc}")
```

![](https://cdn-images-1.medium.com/max/800/1*egeZEnOWbyVKkEpr_5nzAg.png align="center")

Scores AUC avec max\_depth=3

Les scores AUC dâ€™entrainement et de test sont respectivement 68.47% et 68.79%(train\_auc ~ test\_auc). Maintenant, essayons dâ€™augmenter la profondeur maximale Ã  7 et rÃ©pÃ©tons la meme procÃ©dure. On obtient comme scores 74.88% pour le lâ€™entrainement et 74.78% pour le testÂ .

Dâ€™ailleurs, pourquoi ne pas judicieusement calculer ces scores pour diffÃ©rentes valeurs de max\_depth et tracer un graphique pour suivre leur Ã©volution?

```python
# initialiser deux listes pour stocker les scores de train et test
train_auc_scores, test_auc_scores = list(), list()

# iterations pour quelques valeurs de la profondeur maximale(max_depth)
for depth in range(1, 40):
    clf = DecisionTreeClassifier(max_depth=depth)
    clf.fit(X_train, y_train)
    
    # predictions sur le training set et sur le test set
    train_predictions = clf.predict(X_train)
    test_predictions  = clf.predict(X_test)
    
    # caluculer les scores des predictions
    train_auc = roc_auc_score(y_train, train_predictions)
    test_auc  = roc_auc_score(y_test,  test_predictions)
    
    # ajouter les scores aux listes
    train_auc_scores.append(train_auc)
    test_auc_scores.append(test_auc)
```

```python
# tracer deux graphiques en utilisant matplotlib et seaborn
plt.figure(figsize=(10, 5))
sns.set_style("darkgrid")
plt.plot(train_auc_scores, label="train AUC score")
plt.plot(test_auc_scores, label="test AUC score")
plt.legend(loc="upper left", prop={'size': 15})
plt.xticks(range(0, 30, 3))
plt.xlabel("max_depth", size=20)
plt.ylabel("AUC score", size=20)
plt.show()
```

Nous constatons que le meilleur score pour le test set est obtenu lorsque *max\_depth* a une valeur approximative de 7. Au fur et Ã  mesure que nous augmentons la valeur de ce paramÃ¨tre, **le score du test set reste le mÃªme ou se dÃ©grade tandis que le score du train continue dâ€™augmenter.** Cela signifie que notre modÃ¨le continue donc dâ€™apprendre les donnÃ©es dâ€™entrainement avec lâ€™augmentation de max\_depth, mais la performance sur les donnÃ©es de test ne sâ€™amÃ©liore pas (enfin ne sâ€™amÃ©liore plus disons).

**Câ€™est ce quâ€™on appelle lâ€™overfitting.**

![](https://cdn-images-1.medium.com/max/800/1*27jCo850JypxGwLnzX2Ezw.jpeg align="left")

Le modÃ¨le sâ€™adapte parfaitement au train set mais donne des rÃ©sultats trÃ¨s mÃ©diocres sur le test set. Cela signifie que le modÃ¨le apprendra bien les donnÃ©es dâ€™apprentissage mais ne pourra pas gÃ©nÃ©raliser sur des Ã©chantillons non vus. Dans lâ€™ensemble de donnÃ©es ci-dessus, il est possible de construire un modÃ¨le avec un max\_depth trÃ¨s Ã©levÃ© qui obtiendra des rÃ©sultats exceptionnels sur le train set, mais ce type de modÃ¨le nâ€™est pas utile car il ne sera pas gÃ©nÃ©ralisable sur des Ã©chantillons non vus.

### 4\. Une explication plus courante de lâ€™Overfitting

Chaque fois que nous entrainons un rÃ©seau de neuronnes (neural network), nous devons surveiller lâ€™Ã©volution du coÃ»t (loss en anglais) durant lâ€™entrainement Ã  la fois pour le train set et le test set. Si nous avons un trÃ¨s grand rÃ©seau pour un ensemble de donnÃ©es qui est trÃ¨s petit (câ€™est-Ã -dire un trÃ¨s petit nombre dâ€™Ã©chantillons), nous observerons que les coÃ»ts du train et du test set diminuent au fur et Ã  mesure de lâ€™apprentissage.

Cependant, Ã  un moment donnÃ©, le coÃ»t du test set atteindra son minimum, et aprÃ¨s cela, commencera Ã  augmenter mÃªme si le coÃ»t du train set continue de diminuer. Nous devons ainsi arrÃªter lâ€™entrainement lorsque le coÃ»t du test/validation set atteint sa valeur minimale.

![Source: MLearning.ai](https://cdn-images-1.medium.com/max/800/0*p5gD86LO2m3B7C8R.png align="left")

**Câ€™est lâ€™explication la plus courante que vous trouverez sur lâ€™Overfitting.**

Bien, nous avons assez exposÃ© le problÃ¨me de lâ€™overfitting en Machine Learning, Ã  prÃ©sent, ***place aux mesures pour Ã©viter lâ€™overfitting.***

### 5\. Comment Ã©viter lâ€™overfitting?

Quelques mesures que lâ€™on peut mettre en place pour lutter contre lâ€™overfittingÂ :

* **Toujours chercher Ã  acquÃ©rir plus de donnÃ©es:** plus de donnÃ©es, câ€™est synomyne de plus dâ€™informations pour les modÃ¨les qui pourront gÃ©nÃ©raliser correctement.
    
* **Sâ€™assurer de la bonne qualitÃ© des donnÃ©es** que lâ€™on fournit aux modÃ¨les(Garbage In, Garbage Out). La qualitÃ© des donnÃ©es est probablement la clÃ© pour avoir de bonnes performances et eviter lâ€™overfitting. Quelques idÃ©es:
    

âœ” Ã©liminer les caractÃ©ristiques redondantes, inutiles ou fortement corrÃ©lÃ©es pour diminuer le bruit (noise)

âœ” fixer les potentielles erreurs dans le dataset

âœ” traiter correctement les valeurs abÃ©rrantes (outliers) etcâ€¦

* **Simplifier au maximum le modÃ¨le** (on en reparlera dans la section suivante) en utilisant par exemple un modÃ¨le avec peu de paramÃ¨tres ou en contraingnant le modÃ¨le. Par exemple, contraindre un modÃ¨le pour le rendre plus simple et rÃ©duire le risque de surajustement est ce quâ€™on appelle la **RÃ©gularisation**.
    

Il y a principalement trois mÃ©thodes de rÃ©gularisation mais nous ne les aborderons pas dans cet article:

âœ” la RÃ©gularization L1

âœ” la RÃ©gularization L2

âœ” la RÃ©gularization Elastic net

* **Toujours faire une validation croisÃ©e (cross validation)** pour Ã©valuer la performance du modÃ¨le sur diffÃ©rents sous-ensembles de donnÃ©es. Cela permet dâ€™obtenir une estimation plus fiable de la capacitÃ© de gÃ©nÃ©ralisation du modÃ¨le. Quelques exemples de techniques de cross validation:
    

âœ” hold-out cross-validation

âœ” k-fold cross-validation

âœ” stratified k-fold cross-validation

âœ” leave-one-out cross-validation

Notons quâ€™ils y a pleins dâ€™autres mÃ©thodes non mentionnÃ©es comme ***lâ€™ajustement des hyperparamÃ¨tres, lâ€™assemblement (ensembling) de plusieurs modÃ¨les (stacking, bagging, boosting), utilisation de earling stopping, ajout de couches de normalisation et de dropout pour les rÃ©seaux de neuronnes etc etcâ€¦***

Voyons maintenant un principe assez simple qui nous permettra de dÃ©cider si notre modÃ¨le est en Overfitting ou pas.

### **6\. Le principe de simplicitÃ© ou RazoirÂ dâ€™Occam**

> *Le rasoir dâ€™Ockham ou rasoir dâ€™Occam est un principe de raisonnement philosophique entrant dans les concepts de rationalisme et de nominalisme. Ã‰galement appelÃ© principe de simplicitÃ© ou principe de parcimonie, il peut se formuler comme suitÂ :*

> **â€œPluralitas non est ponenda sine necessitateâ€** *(les multiples ne doivent pas Ãªtre utilisÃ©s sans nÃ©cessitÃ©) dont une formulation plus moderne sâ€™apparente Ã  Â«les hypothÃ¨ses suffisantes les plus simples doivent Ãªtre prÃ©fÃ©rÃ©es (il faut et il suffit)Â».*

> [\*SourceÂ : WikipÃ©dia-\*Rasoir dâ€™Ockham](https://fr.wikipedia.org/wiki/Rasoir_d%27Ockham)

Certes nous ne sommes pas en philosophie mais câ€™est un des principes heuristiques fondamentaux en science, sans Ãªtre pour autant Ã  proprement parler un rÃ©sultat scientifique qui sâ€™applique Ã©galement en Machine Learning.

Le rasoir dâ€™Occam dit simplement quâ€™il ne faut pas essayer de compliquer des choses qui peuvent Ãªtre rÃ©solues de maniÃ¨re beaucoup plus simple. En dâ€™autres termes, les solutions les plus simples sont les plus gÃ©nÃ©ralisables.

**En gÃ©nÃ©ral, chaque fois que notre modÃ¨le nâ€™obÃ©it pas au rasoir dâ€™Occam, il sâ€™agit *probablement* dâ€™un cas dâ€™Overfitting.**

![Sourceâ€Šâ€”â€Šhttps://kjtradingsystems.com/](https://cdn-images-1.medium.com/max/800/0*4jWAjyeLJRT-oMgn.jpg align="left")

### **Conclusion**

Lâ€™overfitting est un problÃ¨me frÃ©quent en Machine Learning et savoir lâ€™identifier est un crucial pour construire des modÃ¨les robustes et qui gÃ©nÃ©ralisent bien. Aussi savoir gÃ©rer des cas dâ€™overfitting est un â€œmustâ€ pour tout aspirant Data Scientist/ML Engineer.

En fin de compte pour Ã©viter lâ€™overfitting, toujours se rappeler du rasoir dâ€™Occam et se demander ***Â«Â Pourquoi chercher compliquÃ© quand plus simple suffitÂ ?Â Â».***

Merci de mâ€™avoir lu jusque lÃ  et nâ€™hÃ©site pas Ã  laisser tes remarques et suggestions en commentaire ou Ã  mâ€™envoyer directement un message sur [mon LinkedIn](https://www.linkedin.com/in/anyantudre/).

**Give a clap if you liked et suis moi pour plus de contenu sur la Data Science et le Machine Learning;-)**

R**Ã©fÃ©rences**

ğŸ“š Approaching (Almost) Any Machine Learning Problemâ€Šâ€”â€ŠAbhishek Thakur

ğŸ“š Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow -AurÃ©lien GÃ©ron

ğŸ“š WikipÃ©diaâ€Šâ€”â€ŠRasoir dâ€™Ockham

ğŸ”— Lien du dataset: [**Binary Classification with a Bank Churn Dataset**](https://www.kaggle.com/competitions/playground-series-s4e1/data)